# MIT License
# Copyright 2020 Ryan Hausen
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of
# this software and associated documentation files (the "Software"), to deal in
# the Software without restriction, including without limitation the rights to
# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
# the Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

import gin.tf.external_configurables
import src.features.data_provider
import src.models.losses
import src.models.metric_logging
import src.models.PanopticFastAttention
import src.models.train_model

# tensorflow vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# tf.keras.losses_utils.ReductionV2.NONE == "none"
tf.keras.optimizers.Adam.learning_rate = 0.0001
tf.keras.losses.BinaryCrossentropy.from_logits = True
tf.keras.losses.BinaryCrossentropy.reduction = "none"
#tf.keras.losses.BinaryCrossentropy.label_smoothing = 0.1
tf.keras.losses.CategoricalCrossentropy.from_logits = True
tf.keras.losses.CategoricalCrossentropy.reduction = "none"
tf.keras.losses.MeanAbsoluteError.reduction = "none"
tf.keras.losses.MeanSquaredError.reduction = "none"
# tensorflow ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


# src.features.data_provider vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
#src.features.data_provider.augment.flip_y = True
#src.features.data_provider.augment.flip_x = True
#src.features.data_provider.augment.translate_y = True
#src.features.data_provider.augment.translate_x = True
#src.features.data_provider.augment.rotate = True
#src.features.data_provider.augment.instance_mode = "v8"

src.features.data_provider.get_dataset.batch_size = 64
src.features.data_provider.get_dataset.instance_mode = "v8"
# src.features.data_provider ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


# src.models.losses vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
tf.nn.compute_average_loss.global_batch_size = 64

src.models.losses.semantic_loss.loss_object = @tf.keras.losses.BinaryCrossentropy()
src.models.losses.semantic_loss.avg = @tf.nn.compute_average_loss

src.models.losses.claim_vector_loss.loss_object = @tf.keras.losses.MeanAbsoluteError()
src.models.losses.claim_vector_loss.avg = @tf.nn.compute_average_loss
src.models.losses.claim_vector_loss.weighting = "None"

src.models.losses.discrete_claim_vector_loss.loss_object = @tf.keras.losses.MeanAbsoluteError()
src.models.losses.discrete_claim_vector_loss.avg = @tf.nn.compute_average_loss

src.models.losses.claim_map_loss.loss_object = @tf.keras.losses.CategoricalCrossentropy()
src.models.losses.claim_map_loss.avg = @tf.nn.compute_average_loss
src.models.losses.claim_map_loss.weighting = "None"

src.models.losses.center_of_mass_loss.loss_object = @tf.keras.losses.MeanSquaredError()
src.models.losses.center_of_mass_loss.avg = @tf.nn.compute_average_loss
src.models.losses.center_of_mass_loss.weighting = "None"

src.models.losses.entropy_regularization.weighting = "None"
src.models.losses.entropy_regularization.avg = @tf.nn.compute_average_loss

src.models.losses.loss_function.lambda_semantic = 0
src.models.losses.loss_function.lambda_claim_vector = 0.06
src.models.losses.loss_function.lambda_claim_map = 4
src.models.losses.loss_function.lambda_center_of_mass = 15
src.models.losses.loss_function.lambda_entropy_regularization = 1.0
src.models.losses.loss_function.instance_mode = "v8"

# src.models.losses ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# !!!!!!!!!!!!!!! The above and below lambda values should match !!!!!!!!!!!!!!!

# src.models.metric_logging vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
src.models.metric_logging.update_metrics.lambda_semantic = 0
src.models.metric_logging.update_metrics.lambda_claim_vector = 0.06
src.models.metric_logging.update_metrics.lambda_claim_map = 4
src.models.metric_logging.update_metrics.lambda_center_of_mass = 15
src.models.metric_logging.update_metrics.lambda_entropy_regularization = 1.0
src.models.metric_logging.update_metrics.instance_mode = "v8"
# src.models.metric_logging ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


# src.models.PanopticFastAttention vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
src.models.PanopticFastAttention.get_model.input_shape = [256, 256, 1]
src.models.PanopticFastAttention.get_model.instance_mode = "v9"

src.models.PanopticFastAttention.encoder.input_shape = [256, 256, 1]
src.models.PanopticFastAttention.encoder.filters = [8, 16, 32, 32]
src.models.PanopticFastAttention.encoder.dropout_rate = 0.1

# src.models.PanopticFastAttention.semantic_decoder.output_shape = [256, 256, 4]
# src.models.PanopticFastAttention.semantic_decoder.filters = [32, 64, 128, 256]
# src.models.PanopticFastAttention.semantic_decoder.n_classes = 1

# src.models.PanopticFastAttention.instance_decoder.output_shape = [256, 256, 4]
# src.models.PanopticFastAttention.instance_decoder.filters = [32, 64, 128, 256]

# src.models.PanopticFastAttention.instance_decoder_v2.output_shape = [256, 256, 4]
# src.models.PanopticFastAttention.instance_decoder_v2.filters = [32, 64, 128, 256]
# src.models.PanopticFastAttention.instance_decoder_v2.n = 5

# src.models.PanopticFastAttention.instance_decoder_v3.output_shape = [256, 256, 4]
# src.models.PanopticFastAttention.instance_decoder_v3.filters = [32, 64, 128, 256]

# src.models.PanopticFastAttention.instance_decoder_v4.output_shape = [256, 256, 4]
# src.models.PanopticFastAttention.instance_decoder_v4.filters = [32, 64, 128, 256]
# src.models.PanopticFastAttention.instance_decoder_v4.n = 5

# v5, v6, v7 use the same architecture, but different encoding schemes
# src.models.PanopticFastAttention.instance_decoder_v5.output_shape = [256, 256, 1]
# src.models.PanopticFastAttention.instance_decoder_v5.filters = [16, 32, 64, 128]
# src.models.PanopticFastAttention.instance_decoder_v5.dropout_rate = 0.1
# src.models.PanopticFastAttention.instance_decoder_v5.n_classes = 1
# src.models.PanopticFastAttention.instance_decoder_v5.n_instances = 3

src.models.PanopticFastAttention.instance_decoder_v8.output_shape = [256, 256, 1]
src.models.PanopticFastAttention.instance_decoder_v8.filters = [8, 16, 32, 32]
src.models.PanopticFastAttention.instance_decoder_v8.dropout_rate = 0.1
src.models.PanopticFastAttention.instance_decoder_v8.n_instances = 3

src.models.PanopticFastAttention.instance_decoder_v9.output_shape = [256, 256, 1]
src.models.PanopticFastAttention.instance_decoder_v9.filters = [8, 16, 32, 32]
src.models.PanopticFastAttention.instance_decoder_v9.dropout_rate = 0.1
src.models.PanopticFastAttention.instance_decoder_v9.n_instances = 3
# src.models.PanopticFastAttention ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


# src.models.train_model vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
__main__.training_func.model = @src.models.PanopticFastAttention.get_model
__main__.training_func.optimizer = @tf.keras.optimizers.Adam
__main__.training_func.metric_func = @src.models.metric_logging.update_metrics
__main__.training_func.checkpoint_dir = "../../models"
__main__.training_func.epochs = 2000
__main__.training_func.log_metric_batch_idx = 16
__main__.training_func.model_code_file = "PanopticFastAttention.py"
__main__.training_func.comet_disabled = False
__main__.training_func.experiment_project_name = "morpheus-deblend"
__main__.training_func.experiment_key = None

__main__.step.loss_func = @src.models.losses.loss_function
